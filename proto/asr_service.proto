//some code and comment are copied form:
//https://github.com/GoogleCloudPlatform/java-docs-samples/blob/master/speech/grpc/src/main/java/third_party/google/speech/v1/cloud-speech.proto

//grammar persistence options
// - per alive connection: user opens special channel to service to define grammars, grammars are kept as long as connection is alive
// - predefined per user on disk: service keep predefined grammar as files on disk, loads it at startup
// - auto-cache: all grammars are cached automatically using its hash as ID, max number of cached grammars and prune policy must be defined

// users account manipulation will be provided by different service

syntax = "proto3";

package sarmata;

service ASR
{
   rpc Recognize(stream RecognizeRequest) returns (stream RecognizeResponse);
   rpc DefineGrammar(DefineGrammarRequest) returns (DefineGrammarRespone); // defines user-persistent grammar
   // or some more general method to controll and query account data
}

message RecognizeRequest {
  // The `initial_request` message provides information to the recognizer
  // that specifies how to process the request.
  //
  // The first `RecognizeRequest` message must contain an `initial_request`.
  // Any subsequent `RecognizeRequest` messages must not contain an
  // `initial_request`.
  InitialRecognizeRequest initial_request = 1;

  // The audio data to be recognized. For `NonStreamingRecognize`, all the
  // audio data must be contained in the first (and only) `RecognizeRequest`
  // message. For streaming `Recognize`, sequential chunks of audio data are
  // sent in sequential `RecognizeRequest` messages.
  AudioRequest audio_request = 2;
  
}

message ConfigField
{
    string key = 1;
    string value = 2;
}

message InitialRecognizeRequest {
    repeated ConfigField config = 1;
    string token        = 2;   // or move it to settings
}


// Contains audio data in the format specified in the `InitialRecognizeRequest`.
// Either `content` or `uri` must be supplied. Supplying both or neither
// returns [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT].
message AudioRequest {
  // The audio data bytes encoded as specified in
  // `InitialRecognizeRequest`. Note: as with all bytes fields, protobuffers
  // use a pure binary representation, whereas JSON representations use base64.
  bytes content = 1;

  bool end_of_stream = 2;
}

/////////////

message InterpretRequest
{
  string token    = 1;
  string grammar  = 2;
  string sentence = 3;
}

message InterpretResponse
{
  string error       = 1;
  repeated string si = 2;   // many iterpretation are possible for ambiguous grammars
}

///////////////

message DefineGrammarRequest
{
  string token   = 1;
  string name    = 2;
  string grammar = 3;
}

message DefineGrammarRespone
{
  string error       = 1;
  string grammarName = 2;
}

//////////////


  enum ResponseStatus
  {
    SUCCESS = 0; 
    PARTIAL_MATCH = 1;          // only beginning of utterence recognized
    NO_MATCH = 2;
    NO_INPUT_TIMEOUT = 3;
    RECOGNITION_TIMEOUT = 4; 
    GRAMMAR_LOAD_FAILURE = 5; 
    GRAMMAR_COMPILATION_FAILURE = 6;
    RECOGNIZER_ERROR = 7; 
    TOO_MUCH_SPEECH_TIMEOUT = 8;
    URI_FAILURE = 9; 
    LANGUAGE_UNSUPPORTED = 10;
    CANCELLED = 11;
    SEMANTICS_FAILURE = 12;
    
    START_OF_INPUT = 13;
    END_OF_AUDIO = 14;
  }

// `RecognizeResponse` is the only message type returned to the client.
message RecognizeResponse {
  
  ResponseStatus status = 1;

  // critical error
  string error = 2;
  
  // minor error, ex: error in transcription
  string warning = 3;

  int32 event_time = 4;
  repeated Phrase results = 5;

}

message Phrase {
    message Word {
        string transcript = 1;
        double confidence = 2;
        int32 start = 3;
        int32 end = 4;
        double logprob = 5;
    }

    repeated Word words = 1;
    double confidence = 2;
    bool correct = 3;       // confidence above thresshold
    string semantic_interpretation = 4;
}

///// 






